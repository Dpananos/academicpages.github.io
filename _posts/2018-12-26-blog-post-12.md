---
title: 'The Proposal to Not Lower the P Value Threshold: A Response to Ioannidis'
date: 2018-12-26
permalink: /posts/2018/12/blog-post-12/
tags:
---


In his paper "The Proposal to Lower P Value Thresholds to 0.005", Dr. John Ioannidis argues that the majority of published results in biomedical research are false, and that lowering the p value threshold is one approach to combating the proliferation of false positives in biomedical research.  Dr. Ioannidis claims that p values have three major problems: They are misinterpreted, over trusted, and misused.  The confluence of these three problems leads to the belief, as is argued my Ioannidis, that passing a significance threshold is equivalent with finding a true outcome.  Assumedly, Ioannidis thinks that lowering the p value threshold is a solution to these problems, or at least, will take steps in the appropriate direction to finding a solution.

 I'm writing this blog post in stark disagreement with Dr. Ioannidis.  I will accept his premise that the the problems he outlines exist, but it is immediately clear to practicing statisticians and academics that lowering the threshold for significance by an order of magnitude will not address any of these concerns.  In this blog post, I argue that the three problems Ioannidis mentions are human errors caused by poor statistical pedagogy and the "publish or perish" mentality of academic science.  I will conclude that lowering the statistical significance threshold from 0.05 to 0.005 does not combat Ioannidis' triad, and thus will not ameliorate the proliferation of false positives in medical science research.

Ioannidis mentions in his paper that p values are often erroneously believed to be the probability that the null hypothesis is true. Evidence towards the proposition that p values are poorly understood is provided by FiveThirtyEight, who [once asked several scientists on camera for an interpretation of the p value.  Few gave satisfactory answers](https://fivethirtyeight.com/features/not-even-scientists-can-easily-explain-p-values/).  This perspective is supported by my own experience marking statistics assignments for undergraduate epidemiology students and discussing statistics with my graduate student peers.  Many practitioners, myself included, often *misinterpret* the statistical procedures they employ in their research.  The misinterpretation of statistical procedures is a fault of statistical pedagogy.  I do not intend to present statistical educators as a scapegoat for the proliferation of false positives in biomedical research; Frequentist statistics is counterintuitive, thus hard to understand, and even harder to teach.  Consequently, statistics has taken the backseat to other aspects of biomedical science, being left only as a compulsory full, or even half, credit for graduate students, even though it plays a major part in academic science. Students rarely ask for clarification on understanding of statistical procedures, and instead treat statistics as an algorithmic procedure from which valid inferences are produced. Students never view statistics as a subjective analysis of the data obtained, subject to assumptions and biases imparted by participants and investigators.  Instead, they view it as an algorithmic process from which scientific conclusions are obtained; so long as p<$\alpha$, you have the right answer and can publish your thesis/results because previously published theses/results have p less than the nominal value.  This leads to an *overtrust* in p values as indicators of true effects, and is reinforced by the "publish or perish model" of academic science, which *misuses* p values as an indicator of a successful study, likely do to the misunderstanding of statistics and statistical hypothesis testing.

The misunderstanding of p values, nor the over trust in them, nor their misuse is something which can be addressed by fiddling with the type one error rate. Ioannidis is right that the number of false positives will decrease under this change, but that is nearly a tautology.  Just because the type one error rate is lower does not mean that students will understand statistical procedures better, or will question their results even when they are "statistically significant", and it certainly wont change reviewer's minds when they review a paper which shows p>0.005.

A better proposal to alleviate the problems proposed by Ioannidis is to move from a framework which prioritizes binary decision processes to a framework which prioritizes estimation.  The ASA has taken steps in this direction by suggesting investigators report confidence intervals rather than p values in their papers.  It is my personal opinion that confidence intervals are not sufficient to combat these problems, as a confidence interval still implicitly relies on choice of $\alpha$, which is arbitrary.  At the risk of sounding zealous, I would encourage the adoption of Bayesian methods, especially as methods for fitting Bayesian regression estimators has become incredibly fast, accurate, and accessible.

In conclusion, Ioannidis' intentions are misplaced.  Reducing the significance threshold addresses the symptoms, not the problems, associated with p values.  Salvaging Frequentist statistics will need an overhaul of statistical pedagogy, requiring students to admit that statistics is not automated algorithmic inference, as well as an overhaul of the publishing pipeline for academic work.  Complex problems have complex solutions, and I'm afraid Ioannidis' solution is just a bandaid.
